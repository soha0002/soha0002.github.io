<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
  	<meta name="viewport" content="width=device-width,initial-scale=1">
    <title>home</title>
    <link rel="stylesheet" href="assets/css/reset.css" type="text/css">
  	<link rel="stylesheet" href="assets/css/main.css" type="text/css">
  	<script src="assets/js/jquery-3.4.1.min.js"></script>
  	<script src="assets/js/main.js"></script>
  </head>
  <body>
    <!-- DO NOT REMOVE ABOVE -->
    <!-- start here -->

    <div class="nav">

    </div>

    <div class="main">
      <div class="col1">
        Indeed, the most obvious place to track the prints of myth, magic, and mysticism through contemporary technoculture is, of course, in our fictions. At the beginning of the nineties, geek culture was largely a nerdy niche, its genres and fannish behaviours leagues away from serious cool. But as geeks gained status in the emerging digital economy, the revenge of the nerds was on. The battle is now over, and the nerds rule: popular culture is dominated by superheroes, science fiction, sword and sorcery, RPGs, fanfic, Comicons, Lovecraftmania, cosplay. Geek fandoms have gone thoroughly mainstream, propagated through gaming, Hollywood, online newsfeeds, massive advertising campaigns, and office cubicle decor. With a qualified exception for hard SF, these genres and practices are all interwoven, sometimes ironically, with the sort of occult or otherworldly enchantments tracked in TechGnosis. But it’s not just geek tastes that rule — it’s geek style. As the software analytics company New Relic put it in a recent ad campaign, we are all “data nerds” now. In other words, we like to nerd out on culture that we increasingly experience as data to play with. The in-jokes, scuttlebutt, mash-ups, and lore-obsession of geekery allow us, therefore, to snuggle up to the uncanny possibilities of magic, superpowers, and cosmic evil without ever losing the cover story that makes these pleasures possible for modern folks: that our entertainments are “just fictions,” diversions with no ontological or real psychological upshot, just moves in a game. The funny thing about games and fictions is that they have a weird way of bleeding into reality. Whatever else it is, the world that humans experience is animated with narratives, rituals, and roles that organise psychological experience, social relations, and our imaginative grasp of the material cosmos. The world, then, is in many ways a webwork of fictions, or, better yet, of stories. The contemporary urge to “gamify” our social and technological interactions is, in this sense, simply an extension of the existing games of subculture, of folklore, even of belief. This is the secret truth of the history of religions: not that religions are “nothing more” than fictions, crafted out of sociobiological need or wielded by evil priests to control ignorant populations, but that human reality possesses an inherently fictional or fantastic dimension whose “game engine” can — and will — be organised along variously visionary, banal, and sinister lines. Part of our obsession with counterfactual genres like sci-fi or fantasy is not that they offer escape from reality — most of these genres are glum or dystopian a lot of the time anyway — but because, in reflecting the “as if” character of the world, they are actually realer than they appear. That’s why we have seen the emergence of what scholars call “postmodern religion” between the cracks of our fandoms: emotionally wrenching funerals on World of Warcraft, Mormon (and Scientological) science fictions, Jedi Zen, even Flying Spaghetti Monster parodies that find themselves wrestling with legal definitions of “real” religion.
      </div>
      <div class="col2">
        Perhaps the easiest mode of producing a cheap fake is simply cutting together existing footage and spreading it under false pretences. For example, in April 2018, a falsified BBC report began circulating on WhatsApp, presenting a false story of nuclear escalation between NATO and Russia.56 The shared clip was four minutes long, and featured nuclear mushroom clouds, the Queen’s evacuation from Buckingham Palace, a Russian ship firing missiles, and NATO aircraft being launched in retaliation. As the clip spread, alarmed viewers began sending messages to the BBC, which issued a statement57 explaining that the clip was cut together from YouTube footage uploaded in 2016 by the Benchmarking Assessment Group, an Irish marketing company. The original footage was a 30-minute video designed as a psychometric test for reactions during a disaster scenario. Outside of the focus group, the video was never intended to be taken seriously. But, a still-unknown manipulator was able to re-contextualise and disseminate it through WhatsApp as an imminent threat. WhatsApp’s secure encryption model gives the service an air of authenticity—if one is only receiving information from verified contacts, then everything should be trustworthy. This feeling of authenticity is precisely what gets exploited by manipulators. Once a video like BBC NATO does make it onto a node of one of these hidden social networks, new recipients can feel a political, emotional, or social duty to share information received from their family or friend groups, regardless what judgment they may have about the factual content of the information.58 This and other more prominent recent examples where WhatsApp has been implicated in spreading disinformation illustrate that in the contemporary climate, platforms struggle to moderate problematic content traveling at large scales. 59 Platforms are not equipped to handle fake videos even when these videos are cheap fakes—not generated with sophisticated artificial intelligence. One method platforms have used to attempt to address this is to create encrypted social media networks like WhatsApp that allow sharing between small groups.60 They claim this will promote authentic communication between trusted parties, and thus this technical solution would discourage the spread of disinformation.However, this has also enabled disinformation to enjoy the cover of hidden vitality in which trusted parties in private networks link quickly to wider networks of users while remaining impenetrable to outside oversight.
      </div>
      <div class="col3">
        Algorithmic decision-making is becoming more common every day. Increasingly, important decisions that affect people’s lives are governed by datasets that are too big for an individual to process. People have become accustomed to algorithms making all manner of recommendations, from products to buy, to songs to listen to, to social network connections. But, algorithms are not just recommending, they are also being used to make big decisions about people’s lives. Among many applications, algorithms are used to: • Sort résumés for job applications; • Allocate social services; • Decide who sees advertisements for open positions, housing, and products; • Decide who should be promoted or fired; • Estimate a person’s risk of committing crimes or the length of a prison term; • Assess and allocate insurance and benefits; • Obtain and determine credit; and • Rank and curate news and information in search engines. While algorithmic decision making can offer benefits in terms of speed, efficiency, and even fairness, there is a common misconception that algorithms automatically result in unbiased decisions. While it may appear like algorithms are unbiased calculations because they take in objective points of reference and provide a standard outcome, there remain many problems with those inputs and the outputs. As Frank Pasquale, law professor at the University of Maryland, points out, algorithmic decision-making is “black boxed,” which means that while we may know what goes into the computer for processing and what the outcome is, there are currently no external auditing systems or regulations for assessing what happens to the data during processing.2 Algorithms are attractive because they promise neutrality in decision making—they take in data and deliver results. But algorithms are not “neutral.” In the words of mathematician Cathy O’Neil, an algorithm is an “opinion embedded in mathematics.”3 And like opinions, all algorithms are different. Some algorithms privilege a certain group of people over another. O’Neil argues that across a range of occupations, human decision makers are being encouraged to defer to software systems even when there is evidence that a system is making incorrect, unjust, or harmful decisions. When an algorithm’s output results in unfairness, we refer to it as bias. Bias can find its way into an algorithm in many ways. It can be created through the social context where an algorithm is created, as a result of technical constraints, or by the way the algorithm is used in practice.4 When an algorithm is being created, it is structured by the values of its designer, which might not be neutral. And after an algorithm is created, it must be trained—fed large amounts of data on past decisions—to teach it how to make future decisions. If that training data is itself biased, the algorithm can inherit that bias. For these reasons and others, decisions made by computer are not fundamentally more logical and unbiased than decisions made by people. Black-boxed algorithms can unfairly limit opportunities, restrict services, and even produce “technological redlining.” As Safiya Noble, professor of communication at University of Southern California, writes, technological redlining occurs when algorithms produce inequitable outcomes and replicate known inequalities, leading to the systematic exclusion of Blacks, Latinos, and Native Americans.5 Technological redlining occurs because we have no control over how data is used to profile us. If bias exists in the data, it is replicated in the outcome. Without enforceable mechanisms of transparency, auditing, and accountability, little can be known about how algorithmic decision-making limits or impedes civil rights. Noble writes, “technological redlining is a form of digital data discrimination, which uses our digital identities and activities to bolster inequality and oppression. It is often enacted without our knowledge, through our digital engagements, which become part of algorithmic, automated, and artificially intelligent sorting mechanisms that can either target or exclude us. It is a fundamental dimension of generating, sustaining, or deepening racial, ethnic, and gender discrimination, and it is centrally tied to the distribution of goods and services in society, like education, housing, and other human and civil rights. Technological redlining is closely tied to longstanding practices of ‘redlining,’ which have been consistently defined as illegal by the United States Congress, but which are increasingly elusive because of their digital deployments through online, internet-based software and platforms, including exclusion from, and control over, individual participation and representation in digital systems.”6 Important examples of technological redlining were uncovered by ProPublica, who showed how Facebook’s targeted advertising system allowed for discrimination by race and age.7 These decisions embedded in design have significant ramifications for those who are already marginalised. In this memo, we begin by showcasing one example to illustrate how racial bias manifests in an algorithmic system. We then address the trade-offs between and debates about algorithms and accountability across several key ethical dimensions: fairness and bias; opacity and transparency; the repurposing of data and algorithms; lack of standards for auditing; power and control; as well as trust and expertise. From there, we provide an overview of algorithmic accountability by highlighting how news coverage and self-governance have further exacerbated problems related to unfair, unethical, and possibly illegal applications of algorithmic systems. 
      </div>
      <div class="col4">
        <img src="https://bloximages.chicago2.vip.townnews.com/madison.com/content/tncms/assets/v3/editorial/a/5d/a5dd41fd-9259-53e1-9b2d-24bdabf54bc4/5ed02657edf2a.image.jpg?resize=1200%2C900" alt="">
      </div>
      <div class="col5">
        Data discrimination is a real social problem; Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of color, specifically women of color. Through an analysis of textual and media searches as well as extensive research on paid online advertising, Noble exposes a culture of racism and sexism in the way discoverability is created online. As search engines and their related companies grow in importance—operating as a source for email, a major vehicle for primary and secondary school learning, and beyond—understanding and reversing these disquieting trends and discriminatory practices is of utmost importance. An original, surprising and, at times, disturbing account of bias on the internet, Algorithms of Oppression contributes to our understanding of how racism is created, maintained, and disseminated in the 21st century. In the Fall of 2017, Dr. Safiya Umoja Noble joined the faculty of the University of Southern California (USC) Annenberg School of Communication. Previously, she was an assistant professor in the Department of Information Studies in the Graduate School of Education and Information Studies at UCLA where she held appointments in the Departments of African American Studies, Gender Studies, and Education. She is a partner inStratelligence, a firm that specializes in research on information and data science challenges, and is a co-founder of theInformation Ethics & Equity Institute, which provides training for organizations committed to transforming their information management practices toward more just, ethical, and equitable outcomes. She is the recipient of a Hellman Fellowship and the UCLA Early Career Award.
      </div>

    </div>








    <!-- DO NOT REMOVE BELOW -->
    <!-- end here -->
  </body>
</html>
